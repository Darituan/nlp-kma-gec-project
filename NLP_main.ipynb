{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6374b050",
   "metadata": {},
   "source": [
    "NLP project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d3a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(1, './transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12cc69f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import *\n",
    "from Transformer import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "170a747a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_path = './data/gec-only/train.src.tok'\n",
    "target_path = './data/gec-only/train.tgt.tok'\n",
    "\n",
    "dataset = construct_dataset(input_path, target_path, 856*2, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9968247b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2047"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f0457ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tf.Tensor(\n",
      "[[    0 10431 47489 ...     1     1     1]\n",
      " [    0 47447    13 ...     1     1     1]\n",
      " [    0 25417 17772 ...     1     1     1]\n",
      " ...\n",
      " [    0 36765 25482 ...     1     1     1]\n",
      " [    0 36709 34251 ...     1     1     1]\n",
      " [    0 25417 10965 ...     1     1     1]], shape=(16, 1711), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[10431 47489 50118 ...     1     1     1]\n",
      " [47447    13  1470 ...     1     1     1]\n",
      " [25417 17772 36709 ...     1     1     1]\n",
      " ...\n",
      " [36765 25482 36709 ...     1     1     1]\n",
      " [36709 34251 22063 ...     1     1     1]\n",
      " [25417 10965 41171 ...     1     1     1]], shape=(16, 1711), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for (batch, (input_ids, target_ids)) in enumerate(dataset):\n",
    "    print(batch)\n",
    "    print(input_ids)\n",
    "    print(target_ids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0113b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at C:\\Users\\a.hak/.cache\\huggingface\\hub\\models--distilroberta-base\\snapshots\\d5411c3ee9e1793fd9ef58390b40a80a4c10df32\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\a.hak/.cache\\huggingface\\hub\\models--distilroberta-base\\snapshots\\d5411c3ee9e1793fd9ef58390b40a80a4c10df32\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading file tokenizer.json from cache at C:\\Users\\a.hak/.cache\\huggingface\\hub\\models--distilroberta-base\\snapshots\\d5411c3ee9e1793fd9ef58390b40a80a4c10df32\\tokenizer.json\n",
      "loading configuration file config.json from cache at C:\\Users\\a.hak/.cache\\huggingface\\hub\\models--distilroberta-base\\snapshots\\d5411c3ee9e1793fd9ef58390b40a80a4c10df32\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\a.hak/.cache\\huggingface\\hub\\models--distilroberta-base\\snapshots\\d5411c3ee9e1793fd9ef58390b40a80a4c10df32\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\a.hak/.cache\\huggingface\\hub\\models--distilroberta-base\\snapshots\\d5411c3ee9e1793fd9ef58390b40a80a4c10df32\\model.safetensors\n",
      "Loaded 82,118,400 parameters in the TF 2.0 model.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Transformer model\n",
    "num_layers = 6\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "dff = 2048\n",
    "target_vocab_size = len(dataset)\n",
    "pe_target = 1711\n",
    "rate = 0.01\n",
    "target_vocab_size = len(target_ids)\n",
    "transformer_model = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                                target_vocab_size, pe_target, rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c7b950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c09a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function, optimizer, and other necessary components (as shown in the previous response)\n",
    "\n",
    "# Training step function\n",
    "@tf.function\n",
    "def train_step(input_ids, target_ids):\n",
    "    tar_inp = target_ids[:, :-1]\n",
    "    tar_real = target_ids[:, 1:]\n",
    "\n",
    "    # Create masks\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(input_ids, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer_model(input_ids, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer_model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd2b28d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\a.hak\\AppData\\Local\\Temp\\ipykernel_55008\\3181115009.py\", line 13, in train_step  *\n        predictions, _ = transformer_model(input_ids, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n    File \"C:\\Users\\a.hak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'transformer' (type Transformer).\n    \n    in user code:\n    \n        File \"C:\\Users\\a.hak\\JupiterNotebooks\\nlp\\./transformer\\Transformer.py\", line 21, in call  *\n            dec_output, attention_weights = self.decoder(\n        File \"C:\\Users\\a.hak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        OperatorNotAllowedInGraphError: Exception encountered when calling layer 'decoder' (type Decoder).\n        \n        in user code:\n        \n            File \"C:\\Users\\a.hak\\JupiterNotebooks\\nlp\\transformer\\decoder.py\", line 32, in call  *\n                x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n            File \"C:\\Users\\a.hak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n        \n            OperatorNotAllowedInGraphError: Exception encountered when calling layer 'decoder_layer' (type DecoderLayer).\n            \n            in user code:\n            \n                File \"C:\\Users\\a.hak\\JupiterNotebooks\\nlp\\transformer\\decoder_layer.py\", line 23, in call  *\n                    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n            \n                OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n            \n            \n            Call arguments received by layer 'decoder_layer' (type DecoderLayer):\n              • x=tf.Tensor(shape=(16, 1710, 512), dtype=float32)\n              • enc_output=tf.Tensor(shape=(16, 1711, 768), dtype=float32)\n              • training=True\n              • look_ahead_mask=tf.Tensor(shape=(16, 1, 1710, 1710), dtype=float32)\n              • padding_mask=tf.Tensor(shape=(16, 1, 1, 1711), dtype=float32)\n        \n        \n        Call arguments received by layer 'decoder' (type Decoder):\n          • x=tf.Tensor(shape=(16, 1710), dtype=int32)\n          • enc_output=tf.Tensor(shape=(16, 1711, 768), dtype=float32)\n          • training=True\n          • look_ahead_mask=tf.Tensor(shape=(16, 1, 1710, 1710), dtype=float32)\n          • padding_mask=tf.Tensor(shape=(16, 1, 1, 1711), dtype=float32)\n    \n    \n    Call arguments received by layer 'transformer' (type Transformer):\n      • input_ids=tf.Tensor(shape=(16, 1711), dtype=int32)\n      • target_ids=tf.Tensor(shape=(16, 1710), dtype=int32)\n      • training=True\n      • enc_padding_mask=tf.Tensor(shape=(16, 1, 1, 1711), dtype=float32)\n      • look_ahead_mask=tf.Tensor(shape=(16, 1, 1710, 1710), dtype=float32)\n      • dec_padding_mask=tf.Tensor(shape=(16, 1, 1, 1711), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (batch, (input_ids, target_ids)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):  \u001b[38;5;66;03m# Assuming dataset is prepared\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1200\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1199\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m   1201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    File \"C:\\Users\\a.hak\\AppData\\Local\\Temp\\ipykernel_55008\\3181115009.py\", line 13, in train_step  *\n        predictions, _ = transformer_model(input_ids, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n    File \"C:\\Users\\a.hak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'transformer' (type Transformer).\n    \n    in user code:\n    \n        File \"C:\\Users\\a.hak\\JupiterNotebooks\\nlp\\./transformer\\Transformer.py\", line 21, in call  *\n            dec_output, attention_weights = self.decoder(\n        File \"C:\\Users\\a.hak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        OperatorNotAllowedInGraphError: Exception encountered when calling layer 'decoder' (type Decoder).\n        \n        in user code:\n        \n            File \"C:\\Users\\a.hak\\JupiterNotebooks\\nlp\\transformer\\decoder.py\", line 32, in call  *\n                x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n            File \"C:\\Users\\a.hak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n        \n            OperatorNotAllowedInGraphError: Exception encountered when calling layer 'decoder_layer' (type DecoderLayer).\n            \n            in user code:\n            \n                File \"C:\\Users\\a.hak\\JupiterNotebooks\\nlp\\transformer\\decoder_layer.py\", line 23, in call  *\n                    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n            \n                OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n            \n            \n            Call arguments received by layer 'decoder_layer' (type DecoderLayer):\n              • x=tf.Tensor(shape=(16, 1710, 512), dtype=float32)\n              • enc_output=tf.Tensor(shape=(16, 1711, 768), dtype=float32)\n              • training=True\n              • look_ahead_mask=tf.Tensor(shape=(16, 1, 1710, 1710), dtype=float32)\n              • padding_mask=tf.Tensor(shape=(16, 1, 1, 1711), dtype=float32)\n        \n        \n        Call arguments received by layer 'decoder' (type Decoder):\n          • x=tf.Tensor(shape=(16, 1710), dtype=int32)\n          • enc_output=tf.Tensor(shape=(16, 1711, 768), dtype=float32)\n          • training=True\n          • look_ahead_mask=tf.Tensor(shape=(16, 1, 1710, 1710), dtype=float32)\n          • padding_mask=tf.Tensor(shape=(16, 1, 1, 1711), dtype=float32)\n    \n    \n    Call arguments received by layer 'transformer' (type Transformer):\n      • input_ids=tf.Tensor(shape=(16, 1711), dtype=int32)\n      • target_ids=tf.Tensor(shape=(16, 1710), dtype=int32)\n      • training=True\n      • enc_padding_mask=tf.Tensor(shape=(16, 1, 1, 1711), dtype=float32)\n      • look_ahead_mask=tf.Tensor(shape=(16, 1, 1710, 1710), dtype=float32)\n      • dec_padding_mask=tf.Tensor(shape=(16, 1, 1, 1711), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (input_ids, target_ids)) in enumerate(dataset):  # Assuming dataset is prepared\n",
    "        batch_loss = train_step(input_ids, target_ids)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {total_loss / (batch + 1):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e950a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
